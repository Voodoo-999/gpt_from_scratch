# Simple GPT from Scratch

A super simple GPT (Generative Pretrained Transformer) implemented entirely in a **Google Colab notebook**.  
Train the model on your own text file and generate new text with it.

## How it works
1. Open the Colab notebook.
2. Upload a `.txt` file from your computer.
3. The notebook tokenizes your text and trains a small GPT from scratch.
4. Generate new text using the trained model.
   **(simply run each cell in order)**

## Features
- Train a GPT on any text file you provide.
- Minimal implementation with char level token + positional embeddings and multi-head self-attention.
- Fully in Colab â€” no setup required.

## Notes
- Designed for **educational purposes**.
- Not optimized for production.

